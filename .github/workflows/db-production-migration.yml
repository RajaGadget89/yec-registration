name: Production Database Migration

on:
  push:
    branches: [ main ]
    paths:
      - 'migrations/**'
      - 'supabase/migrations/**'
      - '.github/workflows/db-production-migration.yml'
  workflow_dispatch:
    inputs:
      force_migration:
        description: "Force migration even if no changes detected"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      skip_validation:
        description: "Skip validation steps (not recommended)"
        required: false
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      target_environment:
        description: "Target environment for migration"
        required: true
        default: "production"
        type: choice
        options:
          - "production"
          - "staging"

# Improved concurrency control - prevents cancellation and queues runs
concurrency:
  group: db-migration-${{ github.event.inputs.target_environment || 'production' }}-${{ github.ref_name }}
  cancel-in-progress: false

permissions:
  contents: read
  statuses: write
  pull-requests: write

env:
  SUPABASE_CLI_VERSION: latest

jobs:
  # Pre-flight checks and validation
  preflight_check:
    name: Pre-flight Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      has_migrations: ${{ steps.filter.outputs.migrations }}
      has_changes: ${{ steps.filter.outputs.migrations }}
      migration_count: ${{ steps.count.outputs.count }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect migration changes
        id: filter
        uses: dorny/paths-filter@v3
        with:
          filters: |
            migrations:
              - 'migrations/**/*.sql'
              - 'supabase/migrations/**/*.sql'

      - name: Debug workflow variables
        run: |
          echo "event = ${{ github.event_name }}"
          echo "force = ${{ github.event.inputs.force_migration || 'false' }}"
          echo "target_env = ${{ github.event.inputs.target_environment || 'staging' }}"
          echo "has_changes = ${{ steps.filter.outputs.migrations }}"

      - name: Count migration files
        id: count
        run: |
          count=$(find migrations/ -name "*.sql" -type f | wc -l)
          echo "count=$count" >> $GITHUB_OUTPUT
          echo "Found $count migration files"

      - name: Validate migration files
        if: steps.filter.outputs.migrations == 'true'
        run: |
          echo "Validating migration files..."
          
          # Check for required migration files
          if [ ! -d "migrations" ]; then
            echo "‚ùå Error: migrations/ directory not found"
            exit 1
          fi
          
          # Check migration naming convention
          for file in migrations/*.sql; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              if ! [[ $filename =~ ^[0-9]{14}_.*\.sql$ ]]; then
                echo "‚ùå Error: $filename doesn't follow timestamp naming convention"
                exit 1
              fi
              echo "‚úÖ $filename - valid naming"
            fi
          done
          
          echo "‚úÖ All migration files validated"

      - name: Generate migration summary
        if: steps.filter.outputs.migrations == 'true'
        run: |
          echo "## Migration Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Migration Files**: $(find migrations/ -name "*.sql" -type f | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          
          echo "### Migration Files:" >> $GITHUB_STEP_SUMMARY
          for file in $(ls migrations/*.sql 2>/dev/null | sort); do
            echo "- $(basename "$file")" >> $GITHUB_STEP_SUMMARY
          done

  # Test migrations on shadow database
  test_migrations:
    name: Test Migrations (Shadow DB)
    runs-on: ubuntu-latest
    needs: preflight_check
    if: |
      (needs.preflight_check.outputs.has_migrations == 'true' || github.event.inputs.force_migration == 'true') &&
      github.event.inputs.skip_validation != 'true'
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Supabase CLI
        uses: supabase/setup-cli@v1
        with:
          version: ${{ env.SUPABASE_CLI_VERSION }}

      - name: Start local Supabase (shadow)
        run: |
          echo "Starting local Supabase for testing..."
          supabase start
          supabase status

      - name: Sync migrations to supabase/migrations
        run: |
          echo "Syncing migrations to supabase/migrations directory..."
          
          # Create backup of existing supabase migrations
          if [ -d "supabase/migrations" ]; then
            echo "Backing up existing supabase migrations..."
            cp -r supabase/migrations supabase/migrations.backup
          fi
          
          # Copy local migrations to supabase/migrations
          rm -rf supabase/migrations
          mkdir -p supabase/migrations
          cp -a migrations/*.sql supabase/migrations/
          
          # Restore any existing supabase migrations that aren't in local
          if [ -d "supabase/migrations.backup" ]; then
            echo "Restoring existing supabase migrations..."
            for file in supabase/migrations.backup/*.sql; do
              if [ -f "$file" ]; then
                filename=$(basename "$file")
                if [ ! -f "supabase/migrations/$filename" ]; then
                  echo "Restoring: $filename"
                  cp "$file" supabase/migrations/
                fi
              fi
            done
            rm -rf supabase/migrations.backup
          fi
          
          echo "Migration files synced:"
          ls -la supabase/migrations/

      - name: Apply migrations to shadow database
        env:
          SHADOW_DB_URL: postgresql://postgres:postgres@localhost:54322/postgres
        run: |
          echo "Applying migrations to shadow database..."
          set -euo pipefail
          
          # Reset shadow database and apply all migrations
          printf 'y\n' | supabase db reset --db-url "$SHADOW_DB_URL"
          
          echo "‚úÖ Migrations applied successfully to shadow database"

      - name: Run database tests
        env:
          SHADOW_DB_URL: postgresql://postgres:postgres@localhost:54322/postgres
        run: |
          echo "Running database tests..."
          
          # Basic connectivity test
          psql "$SHADOW_DB_URL" -c "SELECT version();" || exit 1
          
          # Test that all expected tables exist
          psql "$SHADOW_DB_URL" -c "
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_type = 'BASE TABLE'
            ORDER BY table_name;
          " || exit 1
          
          # Test specific tables that should exist
          psql "$SHADOW_DB_URL" -c "
            SELECT 
              CASE WHEN EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'registrations') 
                THEN '‚úÖ registrations table exists' 
                ELSE '‚ùå registrations table missing' 
              END as status;
          "
          
          psql "$SHADOW_DB_URL" -c "
            SELECT 
              CASE WHEN EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'email_outbox') 
                THEN '‚úÖ email_outbox table exists' 
                ELSE '‚ùå email_outbox table missing' 
              END as status;
          "
          
          echo "‚úÖ Database tests passed"

      - name: Stop local Supabase
        if: always()
        run: |
          echo "Stopping local Supabase..."
          supabase stop

  # Deploy to production
  deploy_production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [preflight_check, test_migrations]
    if: |
      (needs.preflight_check.outputs.has_changes == 'true' || github.event.inputs.force_migration == 'true') &&
      github.event_name == 'workflow_dispatch' &&
      github.event.inputs.target_environment == 'production'
    environment:
      name: production
      url: https://yec.rajagadget.live
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # GitHub-hosted runners can't reliably reach IPv6 DB endpoints; direct --db-url fails with "network is unreachable"
      # Supabase recommends --linked for platform projects; --db-url is for self-hosted
      # See: https://supabase.com/docs/reference/cli and https://supabase.com/docs/guides/platform/ipv4
      - name: Validate production secrets
        run: |
          set -euo pipefail
          missing=0
          if [ -z "${{ secrets.SUPABASE_ACCESS_TOKEN }}" ]; then echo "‚ùå SUPABASE_ACCESS_TOKEN is missing"; missing=1; fi
          if [ -z "${{ secrets.SB_PROD_REF }}" ]; then echo "‚ùå SB_PROD_REF is missing"; missing=1; fi
          if [ -z "${{ secrets.PROD_DB_PASSWORD }}" ]; then echo "‚ùå PROD_DB_PASSWORD is missing"; missing=1; fi
          if [ "$missing" -eq 1 ]; then exit 1; fi
          
          echo "‚úÖ Production secrets are present and valid"

      - name: Setup Supabase CLI
        uses: supabase/setup-cli@v1
        with:
          version: ${{ env.SUPABASE_CLI_VERSION }}

      # Using --linked mode to avoid IPv6 connection issues on GitHub-hosted runners
      - name: Link to production project
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        run: |
          set -euo pipefail
          echo "Linking to production project..."
          
          # Small retry logic for project linking
          max_attempts=3
          attempt=1
          
          while [ $attempt -le $max_attempts ]; do
            echo "Link attempt $attempt of $max_attempts..."
            
            if supabase link --project-ref ${{ secrets.SB_PROD_REF }} --password "${{ secrets.PROD_DB_PASSWORD }}" 2>&1; then
              echo "‚úÖ Project linking successful"
              break
            else
              echo "‚ö†Ô∏è  Link attempt $attempt failed"
              if [ $attempt -eq $max_attempts ]; then
                echo "‚ùå All link attempts failed. Please check:"
                echo "1. Database password is correct"
                echo "2. Access token has proper permissions"
                echo "3. Project reference is correct"
                exit 1
              fi
              attempt=$((attempt + 1))
              sleep 2
            fi
          done

      # Safety guard: Quick non-interactive probe to surface any connection issues
      - name: Probe linked connection
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        run: |
          set -euo pipefail
          echo "Probing linked connection..."
          supabase db diff --schema public --linked >/dev/null 2>&1 || echo "warn: linked probe failed (will continue)"

      - name: Sync migrations to supabase/migrations
        run: |
          echo "Syncing migrations to supabase/migrations directory..."
          
          # Create backup of existing supabase migrations
          if [ -d "supabase/migrations" ]; then
            echo "Backing up existing supabase migrations..."
            cp -r supabase/migrations supabase/migrations.backup
          fi
          
          # Copy local migrations to supabase/migrations
          rm -rf supabase/migrations
          mkdir -p supabase/migrations
          cp -a migrations/*.sql supabase/migrations/
          
          # Restore any existing supabase migrations that aren't in local
          if [ -d "supabase/migrations.backup" ]; then
            echo "Restoring existing supabase migrations..."
            for file in supabase/migrations.backup/*.sql; do
              if [ -f "$file" ]; then
                filename=$(basename "$file")
                if [ ! -f "supabase/migrations/$filename" ]; then
                  echo "Restoring: $filename"
                  cp "$file" supabase/migrations/
                fi
              fi
            done
            rm -rf supabase/migrations.backup
          fi
          
          echo "Migration files synced:"
          ls -la supabase/migrations/

      # Using --linked mode to avoid IPv6 connection issues; non-blocking to prevent job failure
      - name: Auto-repair remote_schema (if present)
        timeout-minutes: 3
        continue-on-error: true
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        shell: bash
        run: |
          set -euo pipefail
          echo "Auto-repair: Checking for orphaned remote_schema entries..."
          
          # Re-link to guarantee context for this step (workaround for matrix/runner shells losing state)
          echo "Re-linking to production project..."
          supabase link --project-ref ${{ secrets.SB_PROD_REF }} --password "${{ secrets.PROD_DB_PASSWORD }}" >/dev/null 2>&1 || echo "warn: re-link failed"
          
          # Show diagnostics
          supabase --version
          echo "Using linked mode to avoid IPv6 connection issues"
          
          # Connection smoke test
          supabase db diff --schema public --linked >/dev/null 2>&1 || echo "warn: connection test failed"
          
          # Non-interactive repair
          yes | supabase migration repair --status reverted --linked || echo "repair skipped"
          
          echo "Auto-repair process completed."

      # Using --linked mode to avoid IPv6 connection issues on GitHub-hosted runners
      - name: Generate production migration diff
        id: prod_diff
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        run: |
          set -euo pipefail
          echo "Generating production migration diff..."
          
          # Generate diff and capture output using linked mode
          diff_output=$(supabase db diff --schema public --linked 2>&1 || echo "No diff or error")
          
          # Save diff output for debugging
          echo "$diff_output" > migration_diff.txt
          
          # Check if there are actual changes
          if [ -n "$diff_output" ] && [ "$diff_output" != "No diff or error" ] && [ "$diff_output" != "No schema changes found" ]; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "Changes detected:"
            echo "$diff_output"
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No changes detected"
          fi

      - name: Show migration diff
        if: steps.prod_diff.outputs.has_changes == 'true'
        run: |
          echo "## Migration Changes to be Applied" >> $GITHUB_STEP_SUMMARY
          echo '```sql' >> $GITHUB_STEP_SUMMARY
          cat migration_diff.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      # Regression guard: Prevent accidental re-introduction of direct DB connections
      - name: Verify no direct DB connections in production job
        if: steps.prod_diff.outputs.has_changes == 'true'
        run: |
          set -euo pipefail
          echo "Checking for accidental direct DB connections in production job block..."
          
          # Scope the check to only the production job block to avoid false positives from Shadow/CI jobs
          awk '
            in_block && /^[^[:space:]]/ { exit }     # next top-level job => stop
            /^deploy_production:/ { in_block=1 }     # start
            in_block { print }
          ' .github/workflows/db-production-migration.yml \
          | grep -n -- '--db-url\|SUPABASE_PROD_DB_URL' && {
            echo "‚ùå ERROR: Found direct DB connection usage in production job"
            echo "Production job must use --linked --password mode only to avoid IPv6 connection issues"
            exit 1
          }
          echo "‚úÖ No direct DB connections found in production job"

      # Playbook guard-lint: prevent common non-idempotent SQL from entering prod
      - name: Playbook guard-lint (idempotency/drift-safe checks)
        if: steps.prod_diff.outputs.has_changes == 'true'
        run: |
          set -euo pipefail
          echo "üîé Playbook guard-lint (idempotency/drift-safe checks)"
          bad=0
          
          # Columns without IF NOT EXISTS
          if grep -RIn --include="*.sql" -E "ADD[[:space:]]+COLUMN(?!.*IF[[:space:]]+NOT[[:space:]]+EXISTS)" migrations/; then
            echo "::error::Found ADD COLUMN without IF NOT EXISTS"
            bad=1
          fi
          
          # Indexes without IF NOT EXISTS
          if grep -RIn --include="*.sql" -E "CREATE[[:space:]]+(UNIQUE[[:space:]]+)?INDEX(?!.*IF[[:space:]]+NOT[[:space:]]+EXISTS)" migrations/; then
            echo "::error::Found CREATE INDEX without IF NOT EXISTS"
            bad=1
          fi
          
          # Heuristic for policies: flag plain CREATE POLICY lines (devs must wrap in DO $$ ... pg_policies guard)
          if grep -RIn --include="*.sql" -E "^[[:space:]]*CREATE[[:space:]]+POLICY[[:space:]]" migrations/; then
            echo "::warning::Detected raw CREATE POLICY; ensure it's wrapped with pg_policies guard per Playbook"
          fi
          
          exit $bad

      # Using --linked mode to avoid IPv6 connection issues on GitHub-hosted runners
      - name: Dry run migration
        if: steps.prod_diff.outputs.has_changes == 'true'
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        run: |
          set -euo pipefail
          echo "Performing dry run of migration..."
          supabase db push --linked --dry-run
          echo "‚úÖ Dry run completed successfully"

      # Using --linked mode to avoid IPv6 connection issues on GitHub-hosted runners
      - name: Apply migrations to production
        if: steps.prod_diff.outputs.has_changes == 'true'
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        run: |
          set -euo pipefail
          echo "Pushing migrations to production..."
          supabase db push --linked --dry-run
          supabase db push --linked
          echo "‚úÖ Migrations deployed to production"

      - name: Post-apply diagnostics (read-only)
        if: steps.prod_diff.outputs.has_changes == 'true'
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        run: |
          set -euo pipefail
          echo "Listing migration statuses for diagnostics..."
          # Non-mutating visibility into Supabase CLI's view of migration chain
          supabase migration list --linked || echo "migration list not available"

      - name: No changes to deploy
        if: steps.prod_diff.outputs.has_changes == 'false'
        run: |
          echo "No database changes detected for production. Skipping deployment."
          echo "## No Changes Detected" >> $GITHUB_STEP_SUMMARY
          echo "Production database is already up to date." >> $GITHUB_STEP_SUMMARY

      # Using --linked mode to avoid IPv6 connection issues; verify with no-op dry run
      - name: Verify production deployment (drift-safe)
        if: steps.prod_diff.outputs.has_changes == 'true'
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_NON_INTERACTIVE: "1"
        run: |
          set -euo pipefail
          echo "Verifying production deployment (drift-safe)..."

          # Ensure context (avoid lost link across steps)
          supabase link --project-ref ${{ secrets.SB_PROD_REF }} --password "${{ secrets.PROD_DB_PASSWORD }}"

          # Primary check via diff
          verify_out="$(supabase db diff --schema public --linked 2>&1 || true)"
          echo "${verify_out}" | tee verify_diff.txt

          # Fallback check via dry-run (handles alternative wording like 'Remote database is up to date')
          if ! echo "${verify_out}" | grep -qiE 'No schema changes found|no schema changes|nothing to.*change|nothing to.*diff'; then
            echo "Diff not conclusively empty from db diff, running a no-op dry-run as a secondary check..."
            dry_run_out="$(supabase db push --linked --dry-run 2>&1 || true)"
            echo "${dry_run_out}" | tee -a verify_diff.txt
            if ! echo "${dry_run_out}" | grep -qiE 'Remote database is up to date|0 .* to execute|no migrations.*apply|no changes'; then
              echo "‚ùå Unexpected pending changes after apply:"
              cat verify_diff.txt
              exit 1
            fi
          fi

          echo "‚úÖ Verified: production schema is up to date."

      - name: Upload verify artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: prod-verify-diff
          path: verify_diff.txt
          retention-days: 7

  # Deploy to staging (if requested)
  deploy_staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [preflight_check, test_migrations]
    if: |
      (needs.preflight_check.outputs.has_changes == 'true') &&
      github.event_name == 'push'
    timeout-minutes: 20
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Supabase CLI
        uses: supabase/setup-cli@v1
        with:
          version: ${{ env.SUPABASE_CLI_VERSION }}

      - name: Link to staging project
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
        run: |
          echo "Linking to staging project..."
          supabase link --project-ref ${{ secrets.SB_STAGING_REF }} --password ${{ secrets.STAGING_DB_PASSWORD }}
          echo "‚úÖ Linked to staging project"

      - name: Sync migrations to supabase/migrations
        run: |
          echo "Syncing migrations to supabase/migrations directory..."
          rm -rf supabase/migrations
          mkdir -p supabase/migrations
          cp -a migrations/. supabase/migrations/

      - name: Generate staging migration diff
        id: staging_diff
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
        run: |
          echo "Generating staging migration diff..."
          diff_output=$(supabase db diff --schema public --linked 2>&1 || echo "No diff or error")
          
          if [ -n "$diff_output" ] && [ "$diff_output" != "No diff or error" ] && [ "$diff_output" != "No schema changes found" ]; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Apply migrations to staging
        if: steps.staging_diff.outputs.has_changes == 'true'
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
        run: |
          echo "Applying migrations to staging..."
          supabase db push --dry-run
          supabase db push
          echo "‚úÖ Migrations deployed to staging"

      - name: No staging changes to deploy
        if: steps.staging_diff.outputs.has_changes == 'false'
        run: |
          echo "No database changes detected for staging. Skipping deployment."

  # Generate deployment summary
  deployment_summary:
    name: Deployment Summary
    runs-on: ubuntu-latest
    needs: [preflight_check, test_migrations, deploy_production, deploy_staging]
    if: always()
    timeout-minutes: 5
    steps:
      - name: Generate comprehensive summary
        uses: actions/github-script@v7
        with:
          script: |
            const { data: runs } = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'db-production-migration.yml',
              run_id: context.runId
            });
            
            const currentRun = runs.workflow_runs.find(run => run.id === context.runId);
            
            let summary = `## Production Database Migration Summary\n\n`;
            summary += `**Workflow Run:** ${context.runId}\n`;
            summary += `**Trigger:** ${context.eventName}\n`;
            summary += `**Branch:** ${context.ref}\n`;
            summary += `**Commit:** ${context.sha.substring(0, 7)}\n\n`;
            
            if (context.eventName === 'workflow_dispatch') {
              summary += `**Manual Trigger:** ${context.payload.inputs?.target_environment || 'production'}\n`;
              summary += `**Force Migration:** ${context.payload.inputs?.force_migration || 'false'}\n`;
              summary += `**Skip Validation:** ${context.payload.inputs?.skip_validation || 'false'}\n\n`;
            }
            
            summary += `### Job Results\n\n`;
            
            const jobs = ['preflight_check', 'test_migrations', 'deploy_production', 'deploy_staging'];
            
            for (const jobName of jobs) {
              try {
                const { data: job } = await github.rest.actions.getJobForWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  job_id: currentRun.jobs.find(j => j.name === jobName)?.id
                });
                
                const status = job.conclusion || job.status;
                const emoji = status === 'success' ? '‚úÖ' : status === 'failure' ? '‚ùå' : '‚è≥';
                summary += `${emoji} **${jobName}:** ${status}\n`;
              } catch (error) {
                summary += `‚ùì **${jobName}:** Not found\n`;
              }
            }
            
            summary += `\n### Next Steps\n\n`;
            summary += `- [ ] Verify application functionality in production\n`;
            summary += `- [ ] Monitor application logs for any issues\n`;
            summary += `- [ ] Check database performance\n`;
            summary += `- [ ] Run integration tests\n`;
            
            // Create a comment on PR if this is a PR
            if (context.eventName === 'pull_request') {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body: summary
              });
            }
            
            // Set output for other jobs
            core.setOutput('summary', summary);
